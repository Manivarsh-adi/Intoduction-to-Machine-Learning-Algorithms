{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Fine Food Reviews\n",
    "\n",
    "Amazon Fine Food Reviews consist of fine food reviews from Amazon.<br>\n",
    "\n",
    "    Number of reviews: 568,454<br>\n",
    "    Number of Users: 256,059<br>\n",
    "    Number of products: 74,258<br>\n",
    "    Timespan: Oct 1999 - oct 2012<br>\n",
    "    Number of Attributes: 10<br>\n",
    "\n",
    "Attributes Information<br>\n",
    "\n",
    "1. Id<br>\n",
    "2. ProductId --->Unique id for product<br>\n",
    "3. UserId ---> Unique id for use<br>\n",
    "4. ProfileName<br>\n",
    "5. HelpfullnessNumerator ---> number of the users who founded the review helpful<br>\n",
    "6. HelpfullnessDenominator ---> number of users who indicated whether they found review helpful or not<br>\n",
    "7. Score ---> Rating in between 5 and 1<br>\n",
    "8. Time ---> timestamp for the review<br>\n",
    "9. Summary ---> brief summary of the review<br>\n",
    "10. Text ---> text of the review<br>\n",
    "\n",
    "#### Objective\n",
    "\n",
    "Given a review, determine whether the review is (positive (for score 4 and 5) or negative (for score 1 and 2)).\n",
    "\n",
    "<br>\n",
    "[Q]. How to determine if a review is postive or negative?<br>\n",
    "<br>\n",
    "[Ans]. Using the score attribute scores which are in range of 4 and 5 consider it as positive rating and for scores 1 and 2 consider them as negative rating. This is the approximate way for dividing polarity (pos / neg)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "\n",
    "Dataset is of two types\n",
    "\n",
    "1. csv format\n",
    "2. SQLite Database\n",
    "\n",
    "We work with both the files in order to analysis the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Libraries to analysis the data\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "sn.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  "
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data using sqlite\n",
    "con = sqlite3.connect('database.sqlite')\n",
    "\n",
    "# Select the reviews where the score is not equal to 3\n",
    "\n",
    "filtered_data = pd.read_sql_query(\"\"\" SELECT * FROM Reviews WHERE Score != 3 \"\"\", con)\n",
    "\n",
    "filtered_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 525814 entries, 0 to 525813\n",
      "Data columns (total 10 columns):\n",
      " #   Column                  Non-Null Count   Dtype \n",
      "---  ------                  --------------   ----- \n",
      " 0   Id                      525814 non-null  int64 \n",
      " 1   ProductId               525814 non-null  object\n",
      " 2   UserId                  525814 non-null  object\n",
      " 3   ProfileName             525814 non-null  object\n",
      " 4   HelpfulnessNumerator    525814 non-null  int64 \n",
      " 5   HelpfulnessDenominator  525814 non-null  int64 \n",
      " 6   Score                   525814 non-null  int64 \n",
      " 7   Time                    525814 non-null  int64 \n",
      " 8   Summary                 525814 non-null  object\n",
      " 9   Text                    525814 non-null  object\n",
      "dtypes: int64(5), object(5)\n",
      "memory usage: 40.1+ MB\n"
     ]
    }
   ],
   "source": [
    "filtered_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points are  525814\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Postive</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Postive</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator     Score        Time  \\\n",
       "0                     1                       1   Postive  1303862400   \n",
       "1                     0                       0  Negative  1346976000   \n",
       "2                     1                       1   Postive  1219017600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  "
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Give reivew postive(1) rating for score < 3 and negative(0) rating for score > 3\\\n",
    "\n",
    "def partition(x):\n",
    "    if x < 3:\n",
    "        return \"Negative\"\n",
    "    return \"Postive\"\n",
    "\n",
    "\n",
    "actual_score = filtered_data['Score']\n",
    "PostiveNegative = actual_score.map(partition)\n",
    "filtered_data['Score'] = PostiveNegative\n",
    "print(\"Number of data points are \", filtered_data.shape[0])\n",
    "filtered_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "## [2] Data Cleaning : Data Duplication\n",
    "\n",
    "It is observed that there is duplicate in the data, it is necessary to remove the duplicate values to remove biasness while training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78445</td>\n",
       "      <td>B000HDL1RQ</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>138317</td>\n",
       "      <td>B000HDOPYC</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138277</td>\n",
       "      <td>B000HDOPYM</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73791</td>\n",
       "      <td>B000HDOPZG</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>155049</td>\n",
       "      <td>B000PAQ75C</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   ProductId         UserId      ProfileName  HelpfulnessNumerator  \\\n",
       "0   78445  B000HDL1RQ  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "1  138317  B000HDOPYC  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "2  138277  B000HDOPYM  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "3   73791  B000HDOPZG  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "4  155049  B000PAQ75C  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "\n",
       "   HelpfulnessDenominator  Score        Time  \\\n",
       "0                       2      5  1199577600   \n",
       "1                       2      5  1199577600   \n",
       "2                       2      5  1199577600   \n",
       "3                       2      5  1199577600   \n",
       "4                       2      5  1199577600   \n",
       "\n",
       "                             Summary  \\\n",
       "0  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "1  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "2  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "3  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "4  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "\n",
       "                                                Text  \n",
       "0  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "1  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "2  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "3  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "4  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  "
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display = pd.read_sql_query(\"\"\" SELECT * FROM Reviews WHERE Score != 3  \n",
    "                                 AND UserID = \"AR5J8UI46CURR\"\n",
    "                                  ORDER BY ProductID \"\"\", con)\n",
    "\n",
    "display.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **As we can see above same user had multiple reviews of the with same values of HelpfullNumerator, HelpfullDenominator, Score Time, Summary and text and on doing analysis we found it**<br>\n",
    "\n",
    "* **ProductID = B000HDOPZG was Loacker Quadratini Vanilla Wafer Cookie, 8.82-Ounce (Pack of 8)**<br>\n",
    "\n",
    "* **ProductID = B000HDL1RQ was Loacker Quadratini Lemon Wafer Cookie, 8.82 Ounce (Pack of 8) and so on**<br>\n",
    "\n",
    "* **Reason for same reviews with parameters excluding Product ID are same, this is beacuse  of Seller of the product Loacker Had metioned both the products are same like they are wafer of wafer type, but falovour was diifernt. this lead to same review with same Summary, Text and score of different product.**<br>\n",
    "\n",
    "* **If we neglect those type of duplicate values then it may lead to biasness, where Algorithm while training it bends towards the data that have biasness.**<br>\n",
    "\n",
    "* **Inorder to remove this type of duplicate values first sort them in ascending oreder then remove the duplicate values using pandas pre built method**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting data according ProductID in ascending order\n",
    "\n",
    "sorted_data=filtered_data.sort_values('ProductId', axis = 0, ascending=True, kind = 'quicksort', na_position = 'last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138706</th>\n",
       "      <td>150524</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>ACITT7DI6IDDL</td>\n",
       "      <td>shari zychinski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Postive</td>\n",
       "      <td>939340800</td>\n",
       "      <td>EVERY book is educational</td>\n",
       "      <td>this witty little book makes my son laugh at l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138688</th>\n",
       "      <td>150506</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>A2IW4PEEKO2R0U</td>\n",
       "      <td>Tracy</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Postive</td>\n",
       "      <td>1194739200</td>\n",
       "      <td>Love the book, miss the hard cover version</td>\n",
       "      <td>I grew up reading these Sendak books, and watc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138689</th>\n",
       "      <td>150507</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>A1S4A3IQ2MU7V4</td>\n",
       "      <td>sally sue \"sally sue\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Postive</td>\n",
       "      <td>1191456000</td>\n",
       "      <td>chicken soup with rice months</td>\n",
       "      <td>This is a fun way for children to learn their ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138690</th>\n",
       "      <td>150508</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>AZGXZ2UUK6X</td>\n",
       "      <td>Catherine Hallberg \"(Kate)\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Postive</td>\n",
       "      <td>1076025600</td>\n",
       "      <td>a good swingy rhythm for reading aloud</td>\n",
       "      <td>This is a great little book to read aloud- it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138691</th>\n",
       "      <td>150509</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>A3CMRKGE0P909G</td>\n",
       "      <td>Teresa</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Postive</td>\n",
       "      <td>1018396800</td>\n",
       "      <td>A great way to learn the months</td>\n",
       "      <td>This is a book of poetry about the months of t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId                  ProfileName  \\\n",
       "138706  150524  0006641040   ACITT7DI6IDDL              shari zychinski   \n",
       "138688  150506  0006641040  A2IW4PEEKO2R0U                        Tracy   \n",
       "138689  150507  0006641040  A1S4A3IQ2MU7V4        sally sue \"sally sue\"   \n",
       "138690  150508  0006641040     AZGXZ2UUK6X  Catherine Hallberg \"(Kate)\"   \n",
       "138691  150509  0006641040  A3CMRKGE0P909G                       Teresa   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator    Score        Time  \\\n",
       "138706                     0                       0  Postive   939340800   \n",
       "138688                     1                       1  Postive  1194739200   \n",
       "138689                     1                       1  Postive  1191456000   \n",
       "138690                     1                       1  Postive  1076025600   \n",
       "138691                     3                       4  Postive  1018396800   \n",
       "\n",
       "                                           Summary  \\\n",
       "138706                   EVERY book is educational   \n",
       "138688  Love the book, miss the hard cover version   \n",
       "138689               chicken soup with rice months   \n",
       "138690      a good swingy rhythm for reading aloud   \n",
       "138691             A great way to learn the months   \n",
       "\n",
       "                                                     Text  \n",
       "138706  this witty little book makes my son laugh at l...  \n",
       "138688  I grew up reading these Sendak books, and watc...  \n",
       "138689  This is a fun way for children to learn their ...  \n",
       "138690  This is a great little book to read aloud- it ...  \n",
       "138691  This is a book of poetry about the months of t...  "
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing Duplicate values\n",
    "\n",
    "final = sorted_data.drop_duplicates( subset = {\"UserId\", \"ProfileName\", \"Time\", \"Text\"}, keep = 'first')\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6925890143662968"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking how much of data is still remaining\n",
    "\n",
    "(final['Id'].size*1.0) / (filtered_data['Id'].size*1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "It is observed that HelpfullnessNumerator is less than Helpfullness Denominator, it is impossible in parctical, we should remove those values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = final[ final.HelpfulnessNumerator > final.HelpfulnessDenominator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59301</th>\n",
       "      <td>64422</td>\n",
       "      <td>B000MIDROQ</td>\n",
       "      <td>A161DK06JJMCYF</td>\n",
       "      <td>J. E. Stephens \"Jeanne\"</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Postive</td>\n",
       "      <td>1224892800</td>\n",
       "      <td>Bought This for My Son at College</td>\n",
       "      <td>My son loves spaghetti so I didn't hesitate or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41159</th>\n",
       "      <td>44737</td>\n",
       "      <td>B001EQ55RW</td>\n",
       "      <td>A2V0I904FH7ABY</td>\n",
       "      <td>Ram</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Postive</td>\n",
       "      <td>1212883200</td>\n",
       "      <td>Pure cocoa taste with crunchy almonds inside</td>\n",
       "      <td>It was almost a 'love at first bite' - the per...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id   ProductId          UserId              ProfileName  \\\n",
       "59301  64422  B000MIDROQ  A161DK06JJMCYF  J. E. Stephens \"Jeanne\"   \n",
       "41159  44737  B001EQ55RW  A2V0I904FH7ABY                      Ram   \n",
       "\n",
       "       HelpfulnessNumerator  HelpfulnessDenominator    Score        Time  \\\n",
       "59301                     3                       1  Postive  1224892800   \n",
       "41159                     3                       2  Postive  1212883200   \n",
       "\n",
       "                                            Summary  \\\n",
       "59301             Bought This for My Son at College   \n",
       "41159  Pure cocoa taste with crunchy almonds inside   \n",
       "\n",
       "                                                    Text  \n",
       "59301  My son loves spaghetti so I didn't hesitate or...  \n",
       "41159  It was almost a 'love at first bite' - the per...  "
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we sould not consider those values\n",
    "\n",
    "final = final[ final.HelpfulnessNumerator <= final.HelpfulnessDenominator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datapoints after basic EDA is  364171\n",
      "\n",
      "Number of positive and negative rating \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Postive     307061\n",
       "Negative     57110\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of datapoints after basic EDA is \", final.shape[0])\n",
    "\n",
    "print(\"\\nNumber of positive and negative rating \")\n",
    "final.Score.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librarie to perform NLP text to vector conversion\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import metrics\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "#from gensim.models import Word2Vec\n",
    "#from gensim.models import KeyedVectors\n",
    "\n",
    "import pickle\n",
    "import tqdm\n",
    "import os\n",
    "import re           # Tutorial about regular expressions:: https://pymotw.com/2/re/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [7.2.2] Bag of Words (BoW)\n",
    "\n",
    "1. **Countvectorizer is the library that is used to convert text into vecrtor using Bag of Words technique.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoW\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "final_counts = count_vect.fit_transform(final['Text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(final_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **sparse matrix atype of matrix, atmost elements of the matrix are zeros.**\n",
    "2. **In a vector the word that appears in text are coded to non-zeros value or frequency of the word, wheras the words that are not occuring in the text are coded as zero.**\n",
    "3. **There are 1000  unique words in corpus, then vector lenght is 1000 in a document/review let the words be 10-15 words,so only 15 cells of vector are coded with non zeros and rest 990-985 cells consist of zero values.**\n",
    "4. **Hence matrix returned by countvectorizer is a sparse matrix.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 115281)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_counts.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 115281)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [7.2.3] Text preprocessing\n",
    "\n",
    "Before converting text into vector we must clean the text, it makes text much efficient or meaningful while converting text into vector.\n",
    "\n",
    "1. **Removing HTML tags <br /> break line tags.**\n",
    "2. **Removing punctuations or limited set of special charcters like #, *...etc.**\n",
    "3. **checking if a word is mada up of english word not alpha-numeric.**\n",
    "4. **check to see that length of the word is not greater then 2(it is researched that no adjective is greater than of length 2).**\n",
    "5. **Convert the word to lowercase.**\n",
    "6. **Remove stopwords.**\n",
    "7. **Picking up the stem(parent) word of words they are in superlative degree or any other degree form, we perform stemming by Snowball Steming(it was observed to be better than Porter Steming).**\n",
    "\n",
    "After we collect words used to describe review positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords') # downloading stopwords to overcome error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words in english are  179\n",
      "\n",
      "Stem word of  tasty  is  b'tasti'\n"
     ]
    }
   ],
   "source": [
    "# We use re(RegularExpression) module to remove all of the trash from text\n",
    "# re module find the patterns in data compile method pre compile the pattern and sub method replace the pattern with string \n",
    "# mentioned  by user.\n",
    "\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "print(\"Number of stop words in english are \", len(stop))\n",
    "\n",
    "# initialising the snowball stemmer\n",
    "sno = nltk.stem.SnowballStemmer('english') \n",
    "word = 'tasty'\n",
    "print(\"\\nStem word of \",word,\" is \", sno.stem(word).encode('utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop.remove('not')\n",
    "stop.remove('very')\n",
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning HTML tags\n",
    "\n",
    "1. **Defining function to clean HTML tags.**\n",
    "2. **re.compile method is used to pre compile the pattern.**\n",
    "3. **re.sub method is used to replace the pattern with string mentioned by user in a given sentence.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my name is manivarsh   in my class'"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "def clean_html(sentence):\n",
    "    cleaner = re.compile('<.*?>')\n",
    "    cleaned_text = re.sub(cleaner, ' ', sentence)\n",
    "    return cleaned_text\n",
    "clean_html(\"my name is manivarsh <br /> in my class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.'"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.Text = final.Text.map(clean_html)\n",
    "final.Text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Punctuations\n",
    "\n",
    "1. **clean_punc method removes punctuations in sentence.**\n",
    "2. **Punctuations doesn;t make any sense after converting into vector.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what s your name  are you aware of dogs  in campus '"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_punc(sentence):\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]', r' ', sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]', r' ', cleaned)\n",
    "    return cleaned\n",
    "clean_punc(\"what's your name, are you aware of dogs? in campus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have bought several of the Vitality canned dog food products and have found them all to be of good quality  The product looks more like a stew than a processed meat and it smells better  My Labrador is finicky and she appreciates this product better than  most '"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.Text = final.Text.map(clean_punc)\n",
    "final.Text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29074266"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenght = 0\n",
    "for sentence in final.Text:\n",
    "    lenght += len(sentence.split())\n",
    "lenght"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Removing words with numbers, whose length is less than 2 and stopwords.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Filter(x):\n",
    "    filtered_sentence = []\n",
    "    final_string = []\n",
    "    for word in x.split():\n",
    "        if ((word.isalpha()) & (len(word) > 2) & (word.lower() not in stop)):\n",
    "            s = (sno.stem(word.lower()).encode('utf8'))\n",
    "            filtered_sentence.append(s)\n",
    "    str1 = b\" \".join(filtered_sentence)\n",
    "    final_string.append(str1)\n",
    "    return final_string[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'bought sever vital can dog food product found good qualiti product look like stew process meat smell better labrador finicki appreci product better'"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.Text = final.Text.map(Filter)\n",
    "final.Text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pos_Neg_Analysis(x, y):\n",
    "    all_positive_words = []\n",
    "    all_negative_words = []\n",
    "    i = 0\n",
    "    for sentence in x:\n",
    "        if y.values[i] == \"Postive\":\n",
    "            for word in sentence.split():\n",
    "                all_positive_words.append(word)\n",
    "        if y.values[i] == \"Negative\":\n",
    "            for word in sentence.split():\n",
    "                all_negative_words.append(word)\n",
    "        i += 1\n",
    "    return (all_positive_words, all_negative_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to execution is  249.35561299324036\n"
     ]
    }
   ],
   "source": [
    "analysis = Pos_Neg_Analysis(final.Text, final.Score)\n",
    "end_time = time.time()\n",
    "print(\"Time taken to execution is \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'product arriv label jumbo salt peanut peanut actual small size unsalt not sure error vendor intend repres product jumbo'"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.Text[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [7.2.4] uni-grams, n-grams\n",
    "\n",
    "### Motivation\n",
    "\n",
    "1. **We have seperated positive words and negative words lets analyse them to motivate for n-grams.**\n",
    "2. **Checking out the frequency distribution of words that mostly appears in postive and negative reviews.**\n",
    "3. **FreqDist Module return list of top words that occurs most frequently in positive and negative review.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 words occuring positive reviews are  [(b'not', 146807), (b'like', 139432), (b'tast', 129068), (b'good', 112789), (b'flavor', 109637), (b'love', 107396), (b'great', 103924), (b'use', 103893), (b'one', 96740), (b'product', 91054), (b'veri', 90849), (b'tri', 86803), (b'tea', 83915), (b'coffe', 78830), (b'make', 75111), (b'get', 72130), (b'food', 64811), (b'would', 55759), (b'time', 55275), (b'buy', 54202)]\n",
      "\n",
      "Top 20 words frequent in negative reviews are  [(b'not', 54382), (b'tast', 34592), (b'like', 32331), (b'product', 28227), (b'one', 20569), (b'flavor', 19580), (b'would', 18064), (b'tri', 17756), (b'veri', 17013), (b'use', 15303), (b'good', 15044), (b'coffe', 14720), (b'get', 13786), (b'buy', 13752), (b'order', 12871), (b'food', 12756), (b'tea', 11667), (b'even', 11085), (b'box', 10845), (b'amazon', 10076)]\n"
     ]
    }
   ],
   "source": [
    "freq_dist_pos = nltk.FreqDist(analysis[0])\n",
    "freq_dist_neg = nltk.FreqDist(analysis[1])\n",
    "print(\"Top 20 words occuring positive reviews are \", freq_dist_pos.most_common(20))\n",
    "print(\"\\nTop 20 words frequent in negative reviews are \", freq_dist_neg.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "1. **Words like tast and like are frequent in postive review and negative review.**\n",
    "2. **Reason is that in negative review words like \"not like\" and \"not taste\" occurs combiningly, but count vectorizer accept them as single words like \"not\" and \"like\" or \"taste\".**\n",
    "3. **By deafult count vectorizer accept as uni-grams.**\n",
    "4. **we can overcome this problem by bi-gram.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bi-gram, tri-gram and so on n-gram\n",
    "\n",
    "# removing stopword \"not\" should be avoided before building n-grams\n",
    "count_vect = CountVectorizer(ngram_range = (1, 2))    # Documnetation is in sklearn.feature_extraction\n",
    "final_bigram_counts = count_vect.fit_transform(final.Text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 2863047)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_bigram_counts.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **for uni gram countvectorizer dimensions are about 115k dimensions.**\n",
    "2. **But for bi gram countvectorizer dimension are about 2.9 miilion dimension**\n",
    "3. **ngram_range is (1, 2) it does unigram and bigram if ngram_range id (1,3) then we get unigram, bigram and trigram.**\n",
    "4. **ngram_range = (min_value, max_value)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [7.2.5] TF - IDF (Term Frequency - Inverse Document Frequency) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vect = TfidfVectorizer(ngram_range = (1, 2))\n",
    "final_tf_idf = tf_idf_vect.fit_transform(final.Text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 2863047)"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_tf_idf.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2863047"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the features names after text is converted into vector using tf-idf\n",
    "\n",
    "features = tf_idf_vect.get_feature_names()\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'aa pleas',\n",
       " 'aaa',\n",
       " 'aaa aaa',\n",
       " 'aaa class',\n",
       " 'aaa condit',\n",
       " 'aaa hockey',\n",
       " 'aaa job',\n",
       " 'aaa magazin',\n",
       " 'aaa perfect']"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ten featrues\n",
    "\n",
    "features[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing 3 rd vector of tf-idf vectorizer by converting sparse to array\n",
    "\n",
    "type(final_tf_idf[3,:])\n",
    "final_tf_idf[3,:].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 10 tfidf values in row and return them with their corresponding tfidf values\n",
    "# np.argsort return array of indices after sorting default sorting order is ascending\n",
    "# [::-1]  reverse the elements of array, becuase we require top values, after sorting maximum values are at the end of the array\n",
    "# after reversing considering only top_n values for next operation [:top_n]\n",
    "# top_feats is the list containing tuples of feature name and corresponding tfidf value of top_n values\n",
    "\n",
    "def top_tfidf_feat(row, features, top_n = 25):\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row [i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = [\"features\", \"tfidf\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tfidf = top_tfidf_feat(final_tf_idf[1,:].toarray()[0], features, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>keep page</td>\n",
       "      <td>0.192299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>paperback seem</td>\n",
       "      <td>0.192299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>grew read</td>\n",
       "      <td>0.192299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>movi incorpor</td>\n",
       "      <td>0.192299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>incorpor love</td>\n",
       "      <td>0.192299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>read sendak</td>\n",
       "      <td>0.192299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rosi movi</td>\n",
       "      <td>0.192299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>page open</td>\n",
       "      <td>0.192299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>version paperback</td>\n",
       "      <td>0.192299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>flimsi take</td>\n",
       "      <td>0.192299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>miss hard</td>\n",
       "      <td>0.192299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>realli rosi</td>\n",
       "      <td>0.186352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cover version</td>\n",
       "      <td>0.186352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sendak book</td>\n",
       "      <td>0.186352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>kind flimsi</td>\n",
       "      <td>0.176187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hard cover</td>\n",
       "      <td>0.173926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>book watch</td>\n",
       "      <td>0.170240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>watch realli</td>\n",
       "      <td>0.170240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>howev miss</td>\n",
       "      <td>0.166021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sendak</td>\n",
       "      <td>0.166021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>paperback</td>\n",
       "      <td>0.161802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>hand keep</td>\n",
       "      <td>0.153081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>two hand</td>\n",
       "      <td>0.149508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>rosi</td>\n",
       "      <td>0.148002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>seem kind</td>\n",
       "      <td>0.146967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             features     tfidf\n",
       "0           keep page  0.192299\n",
       "1      paperback seem  0.192299\n",
       "2           grew read  0.192299\n",
       "3       movi incorpor  0.192299\n",
       "4       incorpor love  0.192299\n",
       "5         read sendak  0.192299\n",
       "6           rosi movi  0.192299\n",
       "7           page open  0.192299\n",
       "8   version paperback  0.192299\n",
       "9         flimsi take  0.192299\n",
       "10          miss hard  0.192299\n",
       "11        realli rosi  0.186352\n",
       "12      cover version  0.186352\n",
       "13        sendak book  0.186352\n",
       "14        kind flimsi  0.176187\n",
       "15         hard cover  0.173926\n",
       "16         book watch  0.170240\n",
       "17       watch realli  0.170240\n",
       "18         howev miss  0.166021\n",
       "19             sendak  0.166021\n",
       "20          paperback  0.161802\n",
       "21          hand keep  0.153081\n",
       "22           two hand  0.149508\n",
       "23               rosi  0.148002\n",
       "24          seem kind  0.146967"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It is containing bi grams and unigrams\n",
    "\n",
    "top_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [7.2.6]  Word2Vec\n",
    "\n",
    "1. **word2Vec is done by google data but it is lenght one of size 1.9 gb, where system needs high end sources to compute.**\n",
    "2. **System must have minimum 12 gb ram inorder to compute data og 1.9 gb.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Building word2vec on amazon data\n",
    "\n",
    "1. **performing word2vec on the data where steming is not performed.**\n",
    "2. **So, we are considering the dataset again as a dresh one because previous in pervious dataset we performed stemin.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  "
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data using sqlite\n",
    "con = sqlite3.connect('database.sqlite')\n",
    "\n",
    "# Select the reviews where the score is not equal to 3\n",
    "\n",
    "filtered_data = pd.read_sql_query(\"\"\" SELECT * FROM Reviews WHERE Score != 3 \"\"\", con)\n",
    "\n",
    "filtered_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points are  525814\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Postive</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Postive</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator     Score        Time  \\\n",
       "0                     1                       1   Postive  1303862400   \n",
       "1                     0                       0  Negative  1346976000   \n",
       "2                     1                       1   Postive  1219017600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  "
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Give reivew postive(1) rating for score < 3 and negative(0) rating for score > 3\\\n",
    "\n",
    "def partition(x):\n",
    "    if x < 3:\n",
    "        return \"Negative\"\n",
    "    return \"Postive\"\n",
    "\n",
    "\n",
    "actual_score = filtered_data['Score']\n",
    "PostiveNegative = actual_score.map(partition)\n",
    "filtered_data['Score'] = PostiveNegative\n",
    "print(\"Number of data points are \", filtered_data.shape[0])\n",
    "filtered_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_data=filtered_data.sort_values('ProductId', axis = 0, ascending=True, kind = 'quicksort', na_position = 'last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = sorted_data.drop_duplicates( subset = {\"UserId\", \"ProfileName\", \"Time\", \"Text\"}, keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = final[ final.HelpfulnessNumerator <= final.HelpfulnessDenominator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "list_of_sent = []\n",
    "for sent in final.Text.values:\n",
    "    filtered_sentence = []\n",
    "    sent = clean_html(sent)\n",
    "    for w in sent.split():\n",
    "        for cleaned_words in clean_punc(w).split():\n",
    "            if (cleaned_words.isalpha()):\n",
    "                filtered_sentence.append(cleaned_words.lower())\n",
    "            else:\n",
    "                continue\n",
    "    list_of_sent.append(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this witty little book makes my son laugh at loud. i recite it in the car as we're driving along and he always can sing the refrain. he's learned about whales, India, drooping roses:  i love all the new words this book  introduces and the silliness of it all.  this is a classic book i am  willing to bet my son will STILL be able to recite from memory when he is  in college\n",
      "====================================PROCESSESED===============================\n",
      "['this', 'witty', 'little', 'book', 'makes', 'my', 'son', 'laugh', 'at', 'loud', 'i', 'recite', 'it', 'in', 'the', 'car', 'as', 'we', 're', 'driving', 'along', 'and', 'he', 'always', 'can', 'sing', 'the', 'refrain', 'he', 's', 'learned', 'about', 'whales', 'india', 'drooping', 'i', 'love', 'all', 'the', 'new', 'words', 'this', 'book', 'introduces', 'and', 'the', 'silliness', 'of', 'it', 'all', 'this', 'is', 'a', 'classic', 'book', 'i', 'am', 'willing', 'to', 'bet', 'my', 'son', 'will', 'still', 'be', 'able', 'to', 'recite', 'from', 'memory', 'when', 'he', 'is', 'in', 'college']\n"
     ]
    }
   ],
   "source": [
    "print(final.Text.values[0])\n",
    "print(\"====================================PROCESSESED===============================\")\n",
    "print(list_of_sent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_file = list_of_sent\n",
    "# min_count = 5, ignore the words, where frequency is less than 5\n",
    "# size = Dimensionality of the word vectors\n",
    "# workers = USe these many worker threads to train the model( faster training with multicore threads)\n",
    "\n",
    "word2vec_model = gensim.models.Word2Vec(list_of_sent, min_count = 5, size = 50, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33294\n"
     ]
    }
   ],
   "source": [
    "# list of words in in model\n",
    "\n",
    "words = list(word2vec_model.wv.vocab)\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tastey', 0.9198199510574341),\n",
       " ('yummy', 0.8654152154922485),\n",
       " ('satisfying', 0.8420820832252502),\n",
       " ('filling', 0.8388378620147705),\n",
       " ('delicious', 0.816381573677063),\n",
       " ('flavorful', 0.8044866323471069),\n",
       " ('addicting', 0.7936882972717285),\n",
       " ('tasteful', 0.756055474281311),\n",
       " ('versatile', 0.7468998432159424),\n",
       " ('delish', 0.7440531849861145)]"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similar words related to tatsy , parllel relationship\n",
    "\n",
    "word2vec_model.wv.most_similar('tasty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('resemble', 0.6897900104522705),\n",
       " ('prefer', 0.6688624620437622),\n",
       " ('dislike', 0.6548731923103333),\n",
       " ('mean', 0.6270785331726074),\n",
       " ('overpower', 0.6257542371749878),\n",
       " ('alright', 0.6105120778083801),\n",
       " ('think', 0.6041259765625),\n",
       " ('enjoy', 0.5896117687225342),\n",
       " ('weird', 0.5778809785842896),\n",
       " ('gross', 0.5769407153129578)]"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.most_similar('like')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "along vacat\n"
     ]
    }
   ],
   "source": [
    "count_vect_feat = count_vect.get_feature_names()      # list of words from Bag of Words\n",
    "count_vect_feat.index('like')\n",
    "print(count_vect_feat[64055])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [7.2.7] Average Word2Vec and TF-IDF * word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compute average word2vec of each review\n",
    "\n",
    "# the avg w2v score for each sentence/review is stored in this list\n",
    "sent_vectors = []\n",
    "for sent in list_of_sent:  # for each review and sentence\n",
    "    \n",
    "    # as word vectors are of 50 length so we store them in sent_vec for each in 50\n",
    "    sent_vec = np.zeros(50)\n",
    "    \n",
    "    \n",
    "    # In order to perform average of the word2vec, create a variable that count number of words in the review\n",
    "    word_count = 0\n",
    "    \n",
    "    # for each word in a review/sentence\n",
    "    for word in sent:\n",
    "        try:\n",
    "            vec = word2vec_model.wv[word]\n",
    "            sent_vec += vec\n",
    "            word_count += 1\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    sent_vec /= word_count\n",
    "    sent_vectors.append(sent_vec)\n",
    "print(sent_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  TF-IDF * Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF weighted Word2Vec\n",
    "\n",
    "tfidf_feat = tf_idf_vect.get_feature_names()  # list of unique words/ feature names of tf-idf\n",
    "\n",
    "# final_tf_idf is the spare matrix\n",
    "\n",
    "# the tf-idf w3v for each sentence/reviews is stored inn this list\n",
    "tfidf_sent_vectors = []\n",
    "row = 0\n",
    "\n",
    "# for each sentence/review \n",
    "for sent in list_of_sent:\n",
    "    sent_vec = np.zeros(50) # there are 50 dimension of vector\n",
    "    weight_sum = 0\n",
    "    \n",
    "    # for each word in sentence/review\n",
    "    for word in sent:\n",
    "        try:\n",
    "            vec = word2vec_model.wv[word]\n",
    "            \n",
    "            # obtain the tf-idf of each word in the sentence / review\n",
    "            tfidf = final_tf_idf[row, tfidf_feat.index(word)]\n",
    "            sent_vec += (vec * tfidf)\n",
    "            weight_sum += tf_idf\n",
    "        except:\n",
    "            pass\n",
    "    sent_vec /= weight_sum\n",
    "    tfidf_sent_vectors.append(sent_vec)\n",
    "    row += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
