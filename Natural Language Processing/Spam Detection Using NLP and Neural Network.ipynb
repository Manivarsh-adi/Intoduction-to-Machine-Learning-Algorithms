{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Detection Usuing NLP and Neural and Network\n",
    "\n",
    "1. **Spam file is collection of emails, categorized by spam and not spam.**\n",
    "2. **spam mails are can be malware files, promotions, Not secured links ...etc.**\n",
    "3. **For given dataset, consist of text and response, perform Natural Language Processing to convert text into desired vectors.**\n",
    "4. **Build a model using Neural Network(ANN or CNN) to predict whether a mail is spam or not.**\n",
    "5. **Spam mails are harm in terms of Malware..etc, if we predict whether a mail is spam or not startingly, we can avoid malware attacks at early.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for analysising the data \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.manifold import TSNE\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Libraries for NLP\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1.1] Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Response</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Response                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam = pd.read_csv('SpamCollection', sep = '\\t', header = None)\n",
    "spam.columns = ['Response', 'Message']\n",
    "spam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: Response, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam.Response.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding is the process of converting string Numerical where ML model doesn't understand string values\n",
    "# should convert string values into numericals, Encoding 'harm' = 0 and 'spam' = 1\n",
    "\n",
    "def Encoding(x):\n",
    "    if x == 'ham':\n",
    "        return 0\n",
    "    if x == 'spam':\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Response</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Response                                            Message\n",
       "0         0  Go until jurong point, crazy.. Available only ...\n",
       "1         0                      Ok lar... Joking wif u oni...\n",
       "2         1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3         0  U dun say so early hor... U c already then say...\n",
       "4         0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam['Response'] = spam.Response.map(Encoding)\n",
    "spam.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1.2] Text Processing\n",
    "\n",
    "1. **removing HTML tags, html tags like <br /> are meaning less in terms of spam detection.**\n",
    "2. **Clean punctuations like !,@,#.. .,..etc are trash to sentence or words while performing Conversion text to vector.**\n",
    "3. **Checking that each word in each mail/document are alphabet, apart from alphabets if there are alphanumeric like \"Mani1234@\" should be removed.**\n",
    "4. **Each of the word in mail/document are of length greater than 2, research suggest that there is no adjective of length less than 2.**\n",
    "5. **Converting Higher case to lower case each word in mail / document to avoid duplication of words.**\n",
    "6. **Removing stop words from mails, stopwords are total 173 words like I, we, the, and...etc words.**\n",
    "7. **Steming words like tasty and tastfull words have the stem words like tatsie, we can decrease the size of vector.**\n",
    "8. **Lemmatization spliting sentence/ document into words such that words like New York or Andhra Pradesh splits combinigly, its done by lemmatization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading stopwords\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words in English are  179\n",
      "\n",
      "Stem word for  tasty  is  b'tasti'\n"
     ]
    }
   ],
   "source": [
    "# we use re module to remove trash in document, 're' stands for regular Expression\n",
    "# re module use compile method to findout pattern in  data, and sub method is used to replace the pattern by string.\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "print(\"Number of stop words in English are \", len(stop))\n",
    "\n",
    "\n",
    "# Intializing the snowball stemmer\n",
    "sno = SnowballStemmer('english')\n",
    "word = \"tasty\"\n",
    "print(\"\\nStem word for \",word,\" is \", sno.stem(word).encode('utf8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Cleaning HTML tags\n",
    "\n",
    "1. **Define a function to clean html tags.**\n",
    "2. **re.compile method is used to findout the patterns in data.**\n",
    "3. **re.sub method is used replace the pattern with string metioned by user.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(x):\n",
    "    cleaned = re.compile('<.*?>')\n",
    "    cleaned_text = re.sub(cleaned, r'', x)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam['Message'] = spam.Message.map(clean_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Cleaning punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_punc(x):\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]', r'', x)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]', r'', cleaned)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "Ok lar... Joking wif u oni...\n",
      "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "U dun say so early hor... U c already then say...\n",
      "Nah I don't think he goes to usf, he lives around here though\n"
     ]
    }
   ],
   "source": [
    "for sent in spam.Message.values[:5]:\n",
    "    sent = clean_html(sent)\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam['Message'] = spam.Message.map(clean_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Response</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point crazy Available only in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Response                                            Message\n",
       "0         0  Go until jurong point crazy Available only in ...\n",
       "1         0                            Ok lar Joking wif u oni\n",
       "2         1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3         0        U dun say so early hor U c already then say\n",
       "4         0  Nah I dont think he goes to usf he lives aroun..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering sentence ,removing stop words, considering alphabets, stemming and converting into lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Filter(x):\n",
    "    filtered_sentence = []\n",
    "    final_string = []\n",
    "    for word in x.split():\n",
    "        if (word.isalpha() & (len(word) > 2) & (word.lower() not in stop)):\n",
    "            s = sno.stem(word.lower())\n",
    "            filtered_sentence.append(s)\n",
    "    str1 = \" \".join(filtered_sentence)\n",
    "    final_string.append(str1)\n",
    "    return final_string[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam['Message'] = spam.Message.map(Filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Response</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>jurong point crazi avail bugi great world buff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>lar joke wif oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>free entri wkli comp win cup final tkts may te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>dun say earli hor alreadi say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>nah dont think goe usf live around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Response                                            Message\n",
       "0         0  jurong point crazi avail bugi great world buff...\n",
       "1         0                                   lar joke wif oni\n",
       "2         1  free entri wkli comp win cup final tkts may te...\n",
       "3         0                      dun say earli hor alreadi say\n",
       "4         0          nah dont think goe usf live around though"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spam.Message.values[0].split()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Ham mail words and Spam mail words\n",
    "\n",
    "1. **After text processing , document is cleaned , words remained are more enough to convert a text into vector.**\n",
    "2. **finding the words that are in Ham and storing them in Ham list and spam list.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ham_Spam_Words(x, y):\n",
    "    Ham_words = []\n",
    "    Spam_words = []\n",
    "    i = 0\n",
    "    for sentence in x:\n",
    "        if y.values[i] == 1:\n",
    "            for word in sentence.split():\n",
    "                Spam_words.append(word)\n",
    "        if y.values[i] == 0:\n",
    "            for word in sentence.split():\n",
    "                Ham_words.append(word)\n",
    "        i += 1\n",
    "    return (Ham_words, Spam_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ham_list = Ham_Spam_Words(spam.Message, spam.Response)[0]\n",
    "Spam_list = Ham_Spam_Words(spam.Message, spam.Response)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding out the most frequently used wprds in Ham mails and spam mails.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 words occurs in Ham mails are ::  [('get', 359), ('come', 295), ('call', 288), ('dont', 263), ('like', 244), ('know', 244), ('ill', 240), ('love', 234), ('got', 232), ('good', 225), ('time', 217), ('want', 213), ('day', 213), ('need', 176), ('one', 171), ('go', 166), ('home', 160), ('lor', 160), ('see', 153), ('sorri', 153)]\n",
      "\n",
      "Top 20 words occurs in Spam mails are ::  [('call', 363), ('free', 215), ('text', 137), ('txt', 137), ('mobil', 135), ('claim', 115), ('stop', 115), ('repli', 109), ('prize', 94), ('get', 87), ('week', 85), ('tone', 73), ('servic', 72), ('send', 70), ('new', 69), ('nokia', 68), ('award', 66), ('cash', 62), ('urgent', 62), ('contact', 61)]\n"
     ]
    }
   ],
   "source": [
    "freq_dist_ham = nltk.FreqDist(Ham_list)\n",
    "freq_dist_spam = nltk.FreqDist(Spam_list)\n",
    "print(\"Top 20 words occurs in Ham mails are :: \", freq_dist_ham.most_common(20))\n",
    "print(\"\\nTop 20 words occurs in Spam mails are :: \", freq_dist_spam.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **There are certain words that most frequently occurs in both ham and spam mails.**\n",
    "2. **Something fishy that certain words to be appears in ham and spam .**\n",
    "3. **To make data more efficient, we must apply n-grams technique.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bi-grams, tri-grams and n-grams\n",
    "\n",
    "# words like 'not' and 'very' are also stopwords, but they plays crucial role for sentence orr document\n",
    "\n",
    "count_vect = CountVectorizer(ngram_range = (1, 2))\n",
    "final_bigram_counts = count_vect.fit_transform(spam.Message.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Countvector for bi-gram words  (5572, 33031)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Shape of Countvector for bi-gram words \",final_bigram_counts.get_shape())\n",
    "print(\"\\n\")\n",
    "final_bigram_counts.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF ( Term-Frequency and Inverse-Document-Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer( ngram_range = (1,2))\n",
    "\n",
    "final_tfidf = tfidf_vect.fit_transform(spam.Message.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of TfidfVectorizer is  (5572, 33031)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Shape of TfidfVectorizer is \", final_tfidf.get_shape())\n",
    "print(\"\\n\")\n",
    "final_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33031"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature names of the vector after the text converted to vector\n",
    "\n",
    "feature_names = tfidf_vect.get_feature_names()\n",
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "\n",
      " Third vector of the tf-idf  [[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Prinitng 3 rd vector of tf-idf vectorizer by converting sparse to array\n",
    "\n",
    "print(type(final_tfidf[3,:]))\n",
    "print(\"\\n Third vector of the tf-idf \", final_tfidf[3,:].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing top n features with corresponding tfidf values for certain row\n",
    "\n",
    "def top_tfidf_feat(row, feature, top_n):\n",
    "    sort = np.argsort(row)[::-1][:top_n]\n",
    "    top_feat = [(feature[i], row[i]) for i in sort]\n",
    "    df = pd.DataFrame(top_feat)\n",
    "    df.columns = [\"Feature_name\", \"Tf-Idf_Values\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_name</th>\n",
       "      <th>Tf-Idf_Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>joke wif</td>\n",
       "      <td>0.432939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lar joke</td>\n",
       "      <td>0.432939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wif oni</td>\n",
       "      <td>0.432939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oni</td>\n",
       "      <td>0.388529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>joke</td>\n",
       "      <td>0.329215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wif</td>\n",
       "      <td>0.306793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lar</td>\n",
       "      <td>0.290229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>üll take</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gonna kill</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gonna leav</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gonna last</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gonna head</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>gonna home</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gonna massiv</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>gonna gym</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>gonna googl</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>gonna gibe</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>gonna let</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>gonna miss</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>gonna finish</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>gonna pay</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>gonna pick</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>gonna pop</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>gonna princ</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>gonna ring</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Feature_name  Tf-Idf_Values\n",
       "0       joke wif       0.432939\n",
       "1       lar joke       0.432939\n",
       "2        wif oni       0.432939\n",
       "3            oni       0.388529\n",
       "4           joke       0.329215\n",
       "5            wif       0.306793\n",
       "6            lar       0.290229\n",
       "7       üll take       0.000000\n",
       "8     gonna kill       0.000000\n",
       "9     gonna leav       0.000000\n",
       "10    gonna last       0.000000\n",
       "11    gonna head       0.000000\n",
       "12    gonna home       0.000000\n",
       "13  gonna massiv       0.000000\n",
       "14     gonna gym       0.000000\n",
       "15   gonna googl       0.000000\n",
       "16    gonna gibe       0.000000\n",
       "17     gonna let       0.000000\n",
       "18    gonna miss       0.000000\n",
       "19  gonna finish       0.000000\n",
       "20     gonna pay       0.000000\n",
       "21    gonna pick       0.000000\n",
       "22     gonna pop       0.000000\n",
       "23   gonna princ       0.000000\n",
       "24    gonna ring       0.000000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_feat = top_tfidf_feat(final_tfidf[1,:].toarray()[0], feature_names, 25)\n",
    "top_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(spam.Message.values, min_count = 3, size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of words or features in word2vec after transforming to vector \n",
    "\n",
    "words = list(word2vec_model.wv.vocab)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_sentence = []\n",
    "for sent in spam.Message.values:\n",
    "    filtered_sentence = []\n",
    "    for word in sent.split():\n",
    "        filtered_sentence.append(word)\n",
    "    list_of_sentence.append(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5572"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_of_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jurong point crazi avail bugi great world buffet cine got amor wat\n",
      "\n",
      "\n",
      "=================PROCESSED====================\n",
      "\n",
      "\n",
      "['jurong', 'point', 'crazi', 'avail', 'bugi', 'great', 'world', 'buffet', 'cine', 'got', 'amor', 'wat']\n"
     ]
    }
   ],
   "source": [
    "print(spam.Message.values[0])\n",
    "print(\"\\n\")\n",
    "print(\"=================PROCESSED====================\")\n",
    "print(\"\\n\")\n",
    "print(list_of_sentence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model_2 = Word2Vec(list_of_sentence, min_count = 1, size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6291"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = list(word2vec_model_2.wv.vocab)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6291"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = word2vec_model_2.wv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_dict = {}\n",
    "for i,j in enumerate(words):\n",
    "    words_dict[j] = vectors[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5572"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_of_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.800270080566406"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.zeros((5572, 6291))\n",
    "df = pd.DataFrame(data, columns = words)\n",
    "for row, sent in enumerate(list_of_sentence):\n",
    "    for word in sent:\n",
    "        df.loc[row, word] = words_dict[word]\n",
    "df.loc[0, 'jurong']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('last', 0.9997702240943909),\n",
       " ('collect', 0.9997590780258179),\n",
       " ('start', 0.9997560977935791),\n",
       " ('way', 0.9997544288635254),\n",
       " ('cash', 0.9997499585151672),\n",
       " ('mobil', 0.9997413158416748),\n",
       " ('friend', 0.9997363686561584),\n",
       " ('call', 0.9997330904006958),\n",
       " ('text', 0.9997284412384033),\n",
       " ('cos', 0.9997254014015198)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words releated to holiday parllel relationship\n",
    "\n",
    "word2vec_model_2.wv.most_similar('holiday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('strip', 0.9282799363136292),\n",
       " ('linerent', 0.9249205589294434),\n",
       " ('handsom', 0.9223565459251404),\n",
       " ('iouri', 0.9219058752059937),\n",
       " ('fli', 0.9214571118354797),\n",
       " ('decis', 0.9210811853408813),\n",
       " ('heater', 0.9207569360733032),\n",
       " ('bodi', 0.9196411371231079),\n",
       " ('affair', 0.9193337559700012),\n",
       " ('maintain', 0.9190778732299805)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model_2.wv.most_similar('buffet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average word2vec and TF_IDF * word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
